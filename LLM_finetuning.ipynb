{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sit-xinli/ai-course5/blob/main/LLM_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uh5rwbr4q5Nw"
      },
      "source": [
        "## 大規模言語モデルのファインチューニング\n",
        "LLMが唐詩を書けるように、あなたのLLMを微調整します。.\n",
        "\n",
        "**TODOs**\n",
        "1. スライドを読み、この宿題の目的を確認してください。\n",
        "2. このColabノートをコピーして保存してください。\n",
        "3. このColabノートの手順に従って、LLMを微調整する。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRKf5DA69b3r"
      },
      "source": [
        "## GPUをアクティブにする\n",
        "\n",
        "モデルを微調整するので、この宿題が妥当な時間（1～2時間）でできるように、GPUをアクティブにする必要があります。\n",
        "\n",
        "T4 GPUを使っていることを確認！"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8cXGoOcCLrK"
      },
      "source": [
        "\n",
        "## 事前データ準備\n",
        "Tang_testingdata_ja.jsonとTang_trainingdata_ja.jsonをダウンロード"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4irqfznNAlrZ",
        "collapsed": true,
        "outputId": "a8602ab6-a45e-4588-aa09-5c7b97b84546",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'dataset'...\n",
            "remote: Enumerating objects: 11, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 11 (delta 3), reused 10 (delta 2), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (11/11), 122.93 KiB | 5.34 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n",
            "dataset  sample_data\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "#if not os.path.exists('dataset/'):\n",
        "!git clone https://github.com/sit-xinli/dataset.git\n",
        "!ls /content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hJFrdFQn84M"
      },
      "source": [
        "## パッケージのインストール\n",
        "私たちは、微調整を容易にするために、他の人が作成したよくできたパッケージをインストールし、インポートします。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UoAVpcAELzB"
      },
      "source": [
        "以下のコードブロックの実行にかかる時間は約 **5**分ですが、Colabの状態によって異なる場合があります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kuRMjk0rtWBx",
        "outputId": "889b210d-efc4-4a8f-d484-50d081d49bc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bitsandbytes-0.45.5 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.51.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.6.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.31.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.4.26)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: colorama\n",
            "Successfully installed colorama-0.4.6\n",
            "Collecting fsspec==2025.3.0\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fsspec-2025.3.0\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: datasets\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "Successfully installed datasets-3.6.0\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.31.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)\n",
            "Downloading accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.1/362.1 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: accelerate\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.6.0\n",
            "    Uninstalling accelerate-1.6.0:\n",
            "      Successfully uninstalled accelerate-1.6.0\n",
            "Successfully installed accelerate-1.7.0\n"
          ]
        }
      ],
      "source": [
        "\"\"\" このセルでコードを変更しないことを推奨します。 \"\"\"\n",
        "!pip install bitsandbytes\n",
        "!pip install transformers\n",
        "!pip install peft\n",
        "!pip install sentencepiece\n",
        "!pip install colorama\n",
        "!pip install fsspec==2025.3.0\n",
        "!pip install -U datasets\n",
        "!pip install -U accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OMmuIblEXiU"
      },
      "source": [
        "以下のコードブロックの実行時間は約**20**秒ですが、Colabの状態によって異なる場合があります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZVVG_SQrvFpe"
      },
      "outputs": [],
      "source": [
        "\"\"\" このセルでコードを変更しないことを推奨します。 \"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "import json\n",
        "import warnings\n",
        "import logging\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "#import bitsandbytes as bnb\n",
        "from datasets import load_dataset, load_from_disk\n",
        "import transformers, datasets\n",
        "from peft import PeftModel\n",
        "from colorama import *\n",
        "\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from transformers import GenerationConfig\n",
        "from peft import (\n",
        "    #prepare_model_for_int8_training,\n",
        "    prepare_model_for_kbit_training,\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    get_peft_model_state_dict,\n",
        "    prepare_model_for_kbit_training\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCo1znQhpBdt"
      },
      "source": [
        "## 微調整用データセットのダウンロード"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTaVpMgzp3oC"
      },
      "source": [
        "## 便利な関数の定義"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import google.auth\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "creds, _ = google.auth.default()\n",
        "service = build('oauth2', 'v2', credentials=creds)\n",
        "user_info = service.userinfo().get().execute()\n",
        "email = user_info['email']\n",
        "print(\"Authenticated as:\", email)\n",
        "import hashlib\n",
        "\n",
        "# Hash the email and convert to a number\n",
        "hash_digest = hashlib.sha256(email.encode()).hexdigest()\n",
        "numeric_value = int(hash_digest, 16)\n",
        "\n",
        "\"\"\" このセルでコードを変更しないことを推奨します。 \"\"\"\n",
        "\n",
        "seed = numeric_value % 101\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)"
      ],
      "metadata": {
        "id": "Jy4BPnhU5myO",
        "outputId": "94e5fb7c-b03e-43df-ec5d-4bef3fd49e77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authenticated as: xinli@center.shonan-it.ac.jp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dKjoLO3xtfM1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# トレーニングデータの作成\n",
        "def generate_training_data(data_point):\n",
        "    \"\"\"\n",
        "    (1) 目的\n",
        "        - この関数は、データポイント（入力テキストと出力テキスト）を、モデルが読み取れるトークンに変換するために使用される。\n",
        "    (2) 引数\n",
        "        - data_point: dict。フィールドは \"instruction\"、\"input\"、\"output\"。\n",
        "    (3) 返り値\n",
        "        - モデルの入力トークン、モデルを因果的にするアテンションマスク、対応する出力ターゲットを持つdict\n",
        "    (3) 例：\n",
        "        - フィールド \"instruction\"、\"input\"、\"output \"がすべてstrであるdict、data_point_1を作成した場合、この関数は次のように使うことができる：\n",
        "            formulate_article(データ_point_1)\n",
        "    \"\"\"\n",
        "    # construct full input prompt\n",
        "    prompt = f\"\"\"\\\n",
        "[INST] <<SYS>>\n",
        "あなたは親切なアシスタントだし、唐詩を書くこともうまい。\n",
        "<</SYS>>\n",
        "\n",
        "{data_point[\"instruction\"]}\n",
        "{data_point[\"input\"]}\n",
        "[/INST]\"\"\"\n",
        "\n",
        "    # count the number of input tokens\n",
        "    len_user_prompt_tokens = (\n",
        "        len(\n",
        "            tokenizer(\n",
        "                prompt,\n",
        "                truncation=True,\n",
        "                max_length=CUTOFF_LEN + 1,\n",
        "                padding=\"max_length\",\n",
        "            )[\"input_ids\"]\n",
        "        ) - 1\n",
        "    )\n",
        "    # transform input prompt into tokens\n",
        "    full_tokens = tokenizer(\n",
        "        prompt + \" \" + data_point[\"output\"] + \"</s>\",\n",
        "        truncation=True,\n",
        "        max_length=CUTOFF_LEN + 1,\n",
        "        padding=\"max_length\",\n",
        "    )[\"input_ids\"][:-1]\n",
        "    return {\n",
        "        \"input_ids\": full_tokens,\n",
        "        \"labels\": [-100] * len_user_prompt_tokens\n",
        "        + full_tokens[len_user_prompt_tokens:],\n",
        "        \"attention_mask\": [1] * (len(full_tokens)),\n",
        "    }\n",
        "\n",
        "# 生成された回答の評価\n",
        "def evaluate(instruction, generation_config, max_len, input=\"\", verbose=True):\n",
        "    \"\"\"\n",
        "    (1) 目標\n",
        "        - この関数は、与えられた入力文字列からモデルの出力を得るために使われる。\n",
        "\n",
        "    (2) 引数：\n",
        "        - instruction: str, モデルに何をさせたいかの説明。\n",
        "        - generation_config: transformers.GenerationConfigオブジェクト、モデルの推論に関連するデコードパラメータを指定する。\n",
        "        - max_len: int, モデルの出力の最大長。\n",
        "        - input: str, モデルが命令を解くために必要な入力文字列、デフォルトは\"\"(入力なし)\n",
        "        - verbose: bool, モードの出力を表示するかどうか、デフォルトはTrue\n",
        "    (3) 戻り値\n",
        "        - output: str, 命令と入力に従ったモードの応答\n",
        "    (4) 例\n",
        "        - 命令が \"ABC\"、入力が \"DEF \"で、128トークン以下の回答をモデルに与えたい場合、この関数を次のように使うことができる：\n",
        "            evaluate(instruction=\"ABC\", generation_config=generation_config, max_len=128, input=\"DEF\")\n",
        "\n",
        "    \"\"\"\n",
        "    # construct full input prompt\n",
        "    prompt = f\"\"\"\\\n",
        "[INST] <<SYS>>\n",
        "あなたは親切なアシスタントだし、唐詩を書くこともうまい。\n",
        "<</SYS>>\n",
        "\n",
        "{instruction}\n",
        "{input}\n",
        "[/INST]\"\"\"\n",
        "    # プロンプトのテキストをモデルが必要とする数値表現に変換する。\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].cuda()\n",
        "    # モデルを使って返信を生成する\n",
        "    generation_output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        max_new_tokens=max_len,\n",
        "    )\n",
        "    # 生成された応答をデコードしてプリントアウトする。\n",
        "    for s in generation_output.sequences:\n",
        "        output = tokenizer.decode(s)\n",
        "        output = output.split(\"[/INST]\")[1].replace(\"</s>\", \"\").replace(\"<s>\", \"\").replace(\"Assistant:\", \"\").replace(\"Assistant\", \"\").strip()\n",
        "        if (verbose):\n",
        "            print(output)\n",
        "\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNxuuclCqFf5"
      },
      "source": [
        "## 微調整前のモデルと推論をダウンロードする"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmfEFM7TNuRC"
      },
      "source": [
        "以下のコードブロックの実行時間は、デフォルトの設定を使用した場合、約 **10**分かかりますが、Colabの状態によって異なる場合があります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFq6WXaBnxYq"
      },
      "source": [
        "## 微調整前の推論\n",
        "まず、ファインチューニングなしのモデルで何ができるかを見てみよう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox5QRpz4NiPg"
      },
      "source": [
        "以下のコードブロックの実行時間は、デフォルトの設定を使用した場合、約2分**かかりますが、Colabの状態によって異なる場合があります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "e1sFQbHGn3Bw"
      },
      "outputs": [],
      "source": [
        "\"\"\" このセルでコードを変更しないことを推奨します。 \"\"\"\n",
        "\n",
        "model_name = \"Qwen/Qwen3-0.6B\" # 微調整に使用するモデルを設定する。\n",
        "cache_dir = \"./cache\"\n",
        "\n",
        "nf4_config = BitsAndBytesConfig(\n",
        "   load_in_4bit=True,\n",
        "   bnb_4bit_quant_type=\"nf4\",\n",
        "   bnb_4bit_use_double_quant=True,\n",
        "   bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# 指定されたモデル名またはパスから，事前に学習された言語モデルを読み込みます．\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    cache_dir=cache_dir,\n",
        "    quantization_config=nf4_config,\n",
        "    low_cpu_mem_usage = True,\n",
        "    use_auth_token=False\n",
        ")\n",
        "\n",
        "# トークナイザーを作成し、終了シンボル(eos_token)を設定します。\n",
        "logging.getLogger('transformers').setLevel(logging.ERROR)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    add_eos_token=True,\n",
        "    cache_dir=cache_dir,\n",
        "    #quantization_config=nf4_config\n",
        "    use_auth_token=False\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# モデル推論のためのデコーディング・パラメータの設定\n",
        "max_len = 128\n",
        "generation_config = GenerationConfig(\n",
        "    do_sample=True,\n",
        "    temperature=0.1,\n",
        "    num_beams=1,\n",
        "    top_p=0.3,\n",
        "    no_repeat_ngram_size=3,\n",
        "    pad_token_id=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LyTYIcDOAkO"
      },
      "source": [
        "以下のコードブロックは、デフォルトの設定を使用した場合、実行に約 **1** 分かかりますが、Colab の状態によって異なる場合があります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QJyuoPoO2TCr",
        "outputId": "52b98c32-631d-4f0a-eb97-4ec3725eb54e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 1:\n",
            "モデル入力:\n",
            "以下は唐詩の一行目である。 あなたの知識で判断し、唐詩を簡潔に完成しなさい。\n",
            "\n",
            "会っても別れを告げるのは難しい，東風には力がなく、花はすべて散ってしまった。\n",
            "\n",
            "モデル出力:\n",
            "この詩は、詩の内容をより簡潔にするために、詩を短くした。  \n",
            "詩の文脈を把握し、詩に含まれている名前や人物や物語を簡素に記述し、  \n",
            "詩に必要な情報を簡潔にして、  \n",
            "この唐詩は「東風の力が強いため、東風が花を散った」と簡潔な表現にまとめられます。\n",
            "\n",
            "この詩の名前、人物、物語、テーマ、内容、文脈、テーマの内容、テーマをまとめると、  \n",
            "**「東风の力」**  \n",
            "**\n",
            "--------------------------------------------------------------------------------\n",
            "Example 2:\n",
            "モデル入力:\n",
            "以下は唐詩の一行目である。 あなたの知識で判断し、唐詩を簡潔に完成しなさい。\n",
            "\n",
            "重いカーテンの下で、深い喪に服していた，横になってからの夜は長く、澄んでいる。\n",
            "\n",
            "モデル出力:\n",
            "この詩は、詩の名前は「重いのカーヤントの下に深い喪に着ていた」という詩です。\n",
            "\n",
            "この詩の内容を簡約にまとめると、  \n",
            "**「重くしたカーボンの上に深いた喪にいた、夜は長い」**  \n",
            "となります。\n",
            "\n",
            "この唐詩は「長く夜の深さ」を表しています。\n",
            "\n",
            "この诗の内容は、  \n",
            "「重いたカーリンの夜は深い」  \n",
            "と簡潔でまとめられています。\n",
            "\n",
            "詩の文脈を考慮して、  \n",
            "この唐诗の名\n",
            "--------------------------------------------------------------------------------\n",
            "Example 3:\n",
            "モデル入力:\n",
            "以下は唐詩の一行目である。 あなたの知識で判断し、唐詩を簡潔に完成しなさい。\n",
            "\n",
            "逃亡の果てに追いかける香りの星，禁断の園は驚きに満ちている。\n",
            "\n",
            "モデル出力:\n",
            "この詩は、詩の名前と内容を記述して、簡潔で、一言でまとめ、一観をまとめ、まとめ、整理、整理して、整理し、整理した内容をまとめると、  \n",
            "**「**  \n",
            "**詩名：**  \n",
            "「**逃亡**の果たに追かえる香りにのる星，**禁断**の園に驚き**に溁ちている。**」  \n",
            "**内容：** 這詩は，詩の内容を簡素にまとめると，  \n",
            "****  \n",
            "詩の名称：逃亡  \n",
            "詩\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "\"\"\" このセルでコードを変更しないことを推奨します。 \"\"\"\n",
        "\n",
        "# demo examples\n",
        "test_tang_list = ['会っても別れを告げるのは難しい，東風には力がなく、花はすべて散ってしまった。',\n",
        "                  '重いカーテンの下で、深い喪に服していた，横になってからの夜は長く、澄んでいる。',\n",
        "                  '逃亡の果てに追いかける香りの星，禁断の園は驚きに満ちている。']\n",
        "\n",
        "system_prompt = '以下は唐詩の一行目である。 あなたの知識で判断し、唐詩を簡潔に完成しなさい。'\n",
        "\n",
        "# get the model output for each examples\n",
        "demo_before_finetune = []\n",
        "for tang in test_tang_list:\n",
        "  demo_before_finetune.append(f'モデル入力:\\n{system_prompt}\\n\\n{tang}\\n\\nモデル出力:\\n'+evaluate(system_prompt, generation_config, max_len, tang, verbose = False))\n",
        "\n",
        "# print and store the output to text file\n",
        "for idx in range(len(demo_before_finetune)):\n",
        "  print(f\"Example {idx + 1}:\")\n",
        "  print(demo_before_finetune[idx])\n",
        "  print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Stf_U-9FqPjZ"
      },
      "source": [
        "## 微調整のためのハイパーパラメーターの設定\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2ilhBGhTDtU"
      },
      "outputs": [],
      "source": [
        "\"\"\" このハイパーパラメータで遊んでみることを強くお勧めする。 \"\"\"\n",
        "\n",
        "num_train_data = 1040 # ほとんどの場合, 可能な限り多くのデータを訓練したいでしょう. これにより, モデルがより多様な節を見ることができるようになり, 出力の質が向上しますが, 訓練時間も長くなります.\n",
        "                      # デフォルトのパラメータ(1040)を使用した場合: 微調整に約25分、全セルのフル稼働に約50分かかる。\n",
        "                      # 最大値(5000)を使用した場合: 微調整には約100分かかり, 全セルのフル実行には約120分かかる.\n",
        "\n",
        "\"\"\" これらのハイパーパラメータのいくつかを変更したいかもしれない（必ずしも必要ではない）。 \"\"\"\n",
        "\n",
        "output_dir = \"/content/dataset\"  # 結果を出力するディレクトリを設定する（別のディレクトリに結果を保存したい場合は、ここで変更できますが、デフォルトのサブディレクトリ、つまりGoogleドライブに保存することを強くお勧めします）\n",
        "ckpt_dir = \"./exp1\" # モデルのチェックポイントを保存するディレクトリを設定します（モデルのチェックポイントを別のディレクトリに保存したい場合は、ここで変更できます）。\n",
        "num_epoch = 1 # 学習する総エポック数を設定する（数値が大きいほど学習時間が長くなる。colabの無料版を利用する場合、学習時間が長すぎると切断される可能性があるので注意が必要）。\n",
        "LEARNING_RATE = 3e-4 # 学習率を設定する。\n",
        "\n",
        "\n",
        "\"\"\" このパラメータ設定コードを変更しないことを推奨します。 \"\"\"\n",
        "\n",
        "cache_dir = \"./cache\" # キャッシュディレクトリのパスを設定します。\n",
        "from_ckpt = False # チェックポイントからモデルの重みをロードするかどうか, デフォルトはno.\n",
        "ckpt_name = None # 特定のチェックポイントから重みをロードする際に使用するファイル名、デフォルトはなし。\n",
        "dataset_dir = \"/content/dataset/Tang_trainingdata_ja.json\" # データセットのディレクトリまたはファイルパスを設定します．\n",
        "logging_steps = 20 # 学習ログを出力するステップ数を定義します。\n",
        "save_steps = 65 # モデルを保存するステップ数を設定します。\n",
        "save_total_limit = 3 # モデルのチェックポイントを最大何回保持するかを制御します。\n",
        "report_to = None # 実験的メトリクスを報告する対象を設定します。\n",
        "MICRO_BATCH_SIZE = 4 # マイクロバッチのサイズを定義する\n",
        "BATCH_SIZE = 16 # バッチのサイズを定義する\n",
        "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE # 各マイクロバッチの累積グラデーションステップ数を計算する\n",
        "CUTOFF_LEN = 256 # テキストカットオフの最大長を設定します.\n",
        "LORA_R = 8 # LORA(Layer-wise Random Attention)のR値を設定します.\n",
        "LORA_ALPHA = 16 # LORAのアルファ値を設定します.\n",
        "LORA_DROPOUT = 0.05 # LORAのドロップアウト率を設定する。\n",
        "VAL_SET_SIZE = 0 # バリデーションセットのサイズを設定します。\n",
        "TARGET_MODULES = [\"q_proj\", \"up_proj\", \"o_proj\", \"k_proj\", \"down_proj\", \"gate_proj\", \"v_proj\"] # ターゲットとなるモジュールを設定する。\n",
        "device_map = \"auto\" # デバイスマップを設定。デフォルトは \"auto\"。\n",
        "world_size = int(os.environ.get(\"WORLD_SIZE\", 1)) # 環境変数 \"WORLD_SIZE \"の値を取得、設定されていない場合はデフォルトで1。\n",
        "ddp = world_size != 1 # world_sizeに基づいて分散データ処理(DDP)を使用するかどうかを判断。world_sizeが1の場合、DDPは使用されない。\n",
        "if ddp:\n",
        "    device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
        "    GRADIENT_ACCUMULATION_STEPS = GRADIENT_ACCUMULATION_STEPS // world_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REMmOD6L4tp9"
      },
      "source": [
        "## 微調整開始\n",
        "以下のコードブロックの実行時間は、デフォルト設定を使用した場合、約**10分**かかりますが、Colabの状態によって異なる場合があります。\n",
        "微調整の方法は、以下のサイトを参考：https://www.datacamp.com/tutorial/fine-tuning-qwen3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkOt2hDhR4lG"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6W-xe7h9ti0x"
      },
      "outputs": [],
      "source": [
        "\"\"\" このセルでコードを変更しないことを推奨します。 \"\"\"\n",
        "os.environ[\"WANDB_MODE\"] = \"offline\" # disables online logging\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\" # disables wandb entirely\n",
        "\n",
        "# create the output directory you specify\n",
        "os.makedirs(output_dir, exist_ok = True)\n",
        "os.makedirs(ckpt_dir, exist_ok = True)\n",
        "\n",
        "# 根據 from_ckpt 標誌，從 checkpoint 載入模型權重\n",
        "if from_ckpt:\n",
        "    model = PeftModel.from_pretrained(model, ckpt_name)\n",
        "\n",
        "# 將模型準備好以使用 INT8 訓練\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# 使用 LoraConfig 配置 LORA 模型\n",
        "config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    target_modules=TARGET_MODULES,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model = get_peft_model(model, config)\n",
        "\n",
        "# トークナイザー 的 パディング トークン を 0に設定する\n",
        "tokenizer.pad_token_id = 0\n",
        "\n",
        "# トレーニングデータのロードと処理\n",
        "with open(dataset_dir, \"r\", encoding = \"utf-8\") as f:\n",
        "    data_json = json.load(f)\n",
        "with open(\"tmp_dataset.json\", \"w\", encoding = \"utf-8\") as f:\n",
        "    json.dump(data_json[:num_train_data], f, indent = 2, ensure_ascii = False)\n",
        "\n",
        "data = load_dataset('json', data_files=\"tmp_dataset.json\", download_mode=\"force_redownload\")\n",
        "\n",
        "# 学習データを学習セットと検証セットに分割する（VAL_SET_SIZEが0より大きい場合）\n",
        "if VAL_SET_SIZE > 0:\n",
        "    train_val = data[\"train\"].train_test_split(\n",
        "        test_size=VAL_SET_SIZE, shuffle=True, seed=42\n",
        "    )\n",
        "    train_data = train_val[\"train\"].shuffle().map(generate_training_data)\n",
        "    val_data = train_val[\"test\"].shuffle().map(generate_training_data)\n",
        "else:\n",
        "    train_data = data['train'].shuffle().map(generate_training_data)\n",
        "    val_data = None\n",
        "\n",
        "# トランスフォーマー・トレーナーによるモデル・トレーニング\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=val_data,\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
        "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "        warmup_steps=50,\n",
        "        num_train_epochs=num_epoch,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        fp16=True,  # 混合精度トレーニングの使用\n",
        "        logging_steps=logging_steps,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=save_steps,\n",
        "        output_dir=ckpt_dir,\n",
        "        save_total_limit=save_total_limit,\n",
        "        ddp_find_unused_parameters=False if ddp else None,  # DDPを使用して勾配更新戦略を制御するかどうか\n",
        "        report_to=report_to,\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "# モデルのキャッシュ機能を無効にする\n",
        "model.config.use_cache = False\n",
        "\n",
        "# PyTorchバージョン2.0以降とWindows以外のシステムを使用している場合のモデルのコンパイル\n",
        "if torch.__version__ >= \"2\" and sys.platform != 'win32':\n",
        "    model = torch.compile(model)\n",
        "\n",
        "# モデルトレーニングの開始\n",
        "trainer.train()\n",
        "\n",
        "# 学習済みモデルを指定したディレクトリに保存する。\n",
        "model.save_pretrained(ckpt_dir)\n",
        "\n",
        "# トレーニング中にウェイトが不足する可能性があるという警告メッセージを表示する。\n",
        "print(\"\\n 上記のキーが見つからないという警告は無視してください :)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKeGb8bRqWux"
      },
      "source": [
        "##  テスト\n",
        "微調整は終わった。調整後のモデルをテストしたい。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8I3stApnTWb"
      },
      "source": [
        "まず、保存した微調整済みモデル（チェックポイント）をロードする必要がある。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ag6GvOCe9Ql"
      },
      "outputs": [],
      "source": [
        "\"\"\" このセルでコードを変更しないことを推奨します。 \"\"\"\n",
        "\n",
        "# find all available checkpoints\n",
        "ckpts = []\n",
        "for ckpt in os.listdir(ckpt_dir):\n",
        "    if (ckpt.startswith(\"checkpoint-\")):\n",
        "        ckpts.append(ckpt)\n",
        "\n",
        "# list all the checkpoints\n",
        "ckpts = sorted(ckpts, key = lambda ckpt: int(ckpt.split(\"-\")[-1]))\n",
        "print(\"all available checkpoints:\")\n",
        "print(\" id: checkpoint name\")\n",
        "for (i, ckpt) in enumerate(ckpts):\n",
        "    print(f\"{i:>3}: {ckpt}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khq-LbNlcdfp"
      },
      "outputs": [],
      "source": [
        "\"\"\" チェックポイントを変更したいと思うかもしれないが、必ずしも必要ではない。\"\"\"\n",
        "\n",
        "id_of_ckpt_to_use = -1 # 推論に使用するチェックポイントのID（前のセルの出力に対応）。\n",
        "                        # デフォルト値の-1は, 上記のチェックポイントのリストの中で \"最後から2番目 \"のチェックポイントを指します.\n",
        "                        # 他のチェックポイントを選択したい場合は, -1をリストにあるチェックポイントIDのどれかに変更します.\n",
        "\n",
        "ckpt_name = os.path.join(ckpt_dir, ckpts[id_of_ckpt_to_use])\n",
        "\n",
        "\"\"\" デコード・パラメータを変更する必要があるかもしれないが、必ずしも必要ではない。 \"\"\"\n",
        "\n",
        "# ここでデコードパラメータを調整することができます。デコードパラメータの詳細な説明については、宿題のスライドを参照してください。\n",
        "max_len = 128 # 生成される返信の最大長。\n",
        "temperature = 0.1 # 生成される返信のランダム性を設定。値が小さいほど返信が安定する。\n",
        "top_p = 0.3 # top-p(核)サンプリングのしきい値.\n",
        "# top_k = 5 # top-kの値を調整することで、生成される返答の多様性を高め、繰り返し単語が 生成されないようにする。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTr0DfVBSekD"
      },
      "source": [
        "以下のコードブロックの実行時間は、デフォルト設定を使用した場合、約2分**かかりますが、Colabの状態によって異なる場合があります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wKVPpMVtkql"
      },
      "outputs": [],
      "source": [
        "\"\"\" このセルでコードを変更しないことを推奨します。 \"\"\"\n",
        "\n",
        "test_data_path = \"/content/dataset/Tang_testingdata_ja.json\"\n",
        "output_path = os.path.join(output_dir, \"results.txt\")\n",
        "\n",
        "cache_dir = \"./cache\" # キャッシュディレクトリのパスを設定する.\n",
        "seed = 42 # 結果を再現するためのランダムシードを設定する。\n",
        "no_repeat_ngram_size = 3 # 重複セグメントを生成しないように、no-repeat ngramのサイズを設定する。\n",
        "\n",
        "nf4_config = BitsAndBytesConfig(\n",
        "   load_in_4bit=True,\n",
        "   bnb_4bit_quant_type=\"nf4\",\n",
        "   bnb_4bit_use_double_quant=True,\n",
        "   bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# トークン化器を使用して、モデル名をモデルが読み取り可能な数値表現に変換します。\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    cache_dir=cache_dir,\n",
        "    quantization_config=nf4_config\n",
        ")\n",
        "\n",
        "# 事前学習モデルからモデルをロードし、8ビット整数(INT8)モデルとして設定する。\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=nf4_config,\n",
        "    device_map={'': 0},  # 設定使用的設備，此處指定為 GPU 0\n",
        "    cache_dir=cache_dir\n",
        ")\n",
        "\n",
        "# 指定したチェックポイントからモデルの重みをロードする\n",
        "model = PeftModel.from_pretrained(model, ckpt_name, device_map={'': 0})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vs4_EtnoWA5b"
      },
      "source": [
        "以下のコードブロックの実行時間は、デフォルト設定を使用した場合、約 **4**分かかりますが、Colabの状態によって異なる場合があります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcHoU2X8xRkn"
      },
      "outputs": [],
      "source": [
        "\"\"\" このセルでコードを変更しないことを推奨します。 \"\"\"\n",
        "\n",
        "results = []\n",
        "\n",
        "# ランダム性、ビームサーチ、その他の関連パラメータを含む生成コンフィギュレーションを設定する。\n",
        "generation_config = GenerationConfig(\n",
        "    do_sample=True,\n",
        "    temperature=temperature,\n",
        "    num_beams=1,\n",
        "    top_p=top_p,\n",
        "    # top_k=top_k,\n",
        "    no_repeat_ngram_size=no_repeat_ngram_size,\n",
        "    pad_token_id=2\n",
        ")\n",
        "\n",
        "# テストデータの読み込み\n",
        "with open(test_data_path, \"r\", encoding = \"utf-8\") as f:\n",
        "    test_datas = json.load(f)\n",
        "\n",
        "# 各テストデータに対して予測を行い、結果を保存する。\n",
        "with open(output_path, \"w\", encoding = \"utf-8\") as f:\n",
        "  for (i, test_data) in enumerate(test_datas):\n",
        "      predict = evaluate(test_data[\"instruction\"], generation_config, max_len, test_data[\"input\"], verbose = False)\n",
        "      f.write(f\"{i+1}. {test_data['input']} ★★★ {predict}\\n\")\n",
        "      print(f\"{i+1}. {test_data['input']} ★★★ {predict}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tJT1WINKXV7"
      },
      "source": [
        "## **重要なこと**： 15個の唐詩の結果を提出 .\n",
        "これらの唐詩の結果は \"/content/dataset/results.txt \"にあります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sT78LeWaJM4D"
      },
      "source": [
        "## ファインチューニング・モデルとファインチューニングなしのモデルの比較をご覧ください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tx_mZZAZWdiE"
      },
      "source": [
        "ここで、上の「微調整前の推論」で見たのと同じ例で、我々のモデルがどのようなことができるかをチェックする。\n",
        "\n",
        "以下のコードブロックの実行時間は、デフォルトの設定であれば**40**秒程度ですが、Colabの状態によって異なるかもしれません。ここで、上の「微調整前の推論」で見たのと同じ例で、我々のモデルがどのようなことができるかをチェックする。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtDG2WhIWZlS"
      },
      "outputs": [],
      "source": [
        "# test demo examples\n",
        "test_tang_list = ['会っても別れを告げるのは難しい，東風には力がなく、花はすべて散ってしまった。',\n",
        "                  '重いカーテンの下で、深い喪に服していた，横になってからの夜は長く、澄んでいる。',\n",
        "                  '逃亡の果てに追いかける香りの星，禁断の園は驚きに満ちている。']\n",
        "\n",
        "# get the model output for each examples\n",
        "demo_before_finetune = []\n",
        "for tang in test_tang_list:\n",
        "  demo_before_finetune.append(f'モデル入力:\\n{system_prompt}\\n\\n{tang}\\n\\nモデル出力:\\n'+evaluate(system_prompt, generation_config, max_len, tang, verbose = False))\n",
        "\n",
        "# print and store the output to text file\n",
        "for idx in range(len(demo_before_finetune)):\n",
        "  print(f\"Example {idx + 1}:\")\n",
        "  print(demo_before_finetune[idx])\n",
        "  print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wagFeDX4Hpa8"
      },
      "source": [
        "## **重要**： 上記の3つの例は提出しないでください。\n",
        "この3つの例は、微調整前と微調整後のモデルのパフォーマンスを比較するためだけに使用されます。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWgTS13bgnWQ"
      },
      "source": [
        "## 結果のあなたのPCにダウンロードします（ダウンロードフォルダにresult.txtファイル）\n",
        "宿題を終わらせるには、このファイルが必要です。 ブラウザが自動的にダウンロードしない場合は、Google Driveにあります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUDuNQOY4os_"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download(output_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}