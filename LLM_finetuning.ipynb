{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sit-xinli/ai-course5/blob/main/LLM_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uh5rwbr4q5Nw"
      },
      "source": [
        "## 大規模言語モデルのファインチューニング\n",
        "LLMが唐詩を書けるように、あなたのLLMを微調整します。.\n",
        "\n",
        "**TODOs**\n",
        "1. スライドを読み、この宿題の目的を確認してください。\n",
        "2. このColabノートをコピーして保存してください。\n",
        "3. このColabノートの手順に従って、LLMを微調整する。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRKf5DA69b3r"
      },
      "source": [
        "## GPUをアクティブにする\n",
        "\n",
        "モデルを微調整するので、この宿題が妥当な時間（1～2時間）でできるように、GPUをアクティブにする必要があります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrXWZefE-WXJ"
      },
      "source": [
        "## グーグルドライブをマウントする\n",
        "Googleドライブに結果を保存できるように、Googleドライブをマウントする必要があります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8cXGoOcCLrK"
      },
      "source": [
        "以下のコードブロックの実行時間は約***1分**ですが、ColabとGoogle Driveの状態によって異なる場合があります。\n",
        "\n",
        "### 事前データ準備\n",
        "\n",
        "https://github.com/sit-xinli/ai-course5からデータファイルTang_testingdata_ja.jsonとTang_trainingdata_ja.jsonをダウンロードして、Google Driveにアップロードしてください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4irqfznNAlrZ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hJFrdFQn84M"
      },
      "source": [
        "## パッケージのインストール\n",
        "私たちは、微調整を容易にするために、他の人が作成したよくできたパッケージをインストールし、インポートします。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UoAVpcAELzB"
      },
      "source": [
        "以下のコードブロックの実行にかかる時間は約 **5**分ですが、Colabの状態によって異なる場合があります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuRMjk0rtWBx"
      },
      "outputs": [],
      "source": [
        "\"\"\" このセルでコードを変更しないことを推奨します。 \"\"\"\n",
        "!pip install bitsandbytes\n",
        "!pip install transformers\n",
        "!pip install peft\n",
        "!pip install sentencepiece\n",
        "!pip install colorama\n",
        "!pip install fsspec==2025.3.0\n",
        "!pip install -U datasets\n",
        "!pip install -U accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OMmuIblEXiU"
      },
      "source": [
        "以下のコードブロックの実行時間は約**20**秒ですが、Colabの状態によって異なる場合があります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVVG_SQrvFpe"
      },
      "outputs": [],
      "source": [
        "\"\"\" このセルでコードを変更しないことを推奨します。 \"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "import json\n",
        "import warnings\n",
        "import logging\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "#import bitsandbytes as bnb\n",
        "from datasets import load_dataset, load_from_disk\n",
        "import transformers, datasets\n",
        "from peft import PeftModel\n",
        "from colorama import *\n",
        "\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from transformers import GenerationConfig\n",
        "from peft import (\n",
        "    #prepare_model_for_int8_training,\n",
        "    prepare_model_for_kbit_training,\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    get_peft_model_state_dict,\n",
        "    prepare_model_for_kbit_training\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCo1znQhpBdt"
      },
      "source": [
        "## 微調整用データセットのダウンロード"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTaVpMgzp3oC"
      },
      "source": [
        "## 便利な関数の定義"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import google.auth\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "creds, _ = google.auth.default()\n",
        "service = build('oauth2', 'v2', credentials=creds)\n",
        "user_info = service.userinfo().get().execute()\n",
        "email = user_info['email']\n",
        "print(\"Authenticated as:\", email)\n",
        "import hashlib\n",
        "\n",
        "# Hash the email and convert to a number\n",
        "hash_digest = hashlib.sha256(email.encode()).hexdigest()\n",
        "numeric_value = int(hash_digest, 16)\n",
        "\n",
        "\"\"\" このセルでコードを変更しないことを推奨します。 \"\"\"\n",
        "\n",
        "seed = numeric_value % 101\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)"
      ],
      "metadata": {
        "id": "Jy4BPnhU5myO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKjoLO3xtfM1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# トレーニングデータの作成\n",
        "def generate_training_data(data_point):\n",
        "    \"\"\"\n",
        "    (1) 目的\n",
        "        - この関数は、データポイント（入力テキストと出力テキスト）を、モデルが読み取れるトークンに変換するために使用される。\n",
        "    (2) 引数\n",
        "        - data_point: dict。フィールドは \"instruction\"、\"input\"、\"output\"。\n",
        "    (3) 返り値\n",
        "        - モデルの入力トークン、モデルを因果的にするアテンションマスク、対応する出力ターゲットを持つdict\n",
        "    (3) 例：\n",
        "        - フィールド \"instruction\"、\"input\"、\"output \"がすべてstrであるdict、data_point_1を作成した場合、この関数は次のように使うことができる：\n",
        "            formulate_article(データ_point_1)\n",
        "    \"\"\"\n",
        "    # construct full input prompt\n",
        "    prompt = f\"\"\"\\\n",
        "[INST] <<SYS>>\n",
        "あなたは親切なアシスタントだし、唐詩を書くこともうまい。\n",
        "<</SYS>>\n",
        "\n",
        "{data_point[\"instruction\"]}\n",
        "{data_point[\"input\"]}\n",
        "[/INST]\"\"\"\n",
        "\n",
        "    # count the number of input tokens\n",
        "    len_user_prompt_tokens = (\n",
        "        len(\n",
        "            tokenizer(\n",
        "                prompt,\n",
        "                truncation=True,\n",
        "                max_length=CUTOFF_LEN + 1,\n",
        "                padding=\"max_length\",\n",
        "            )[\"input_ids\"]\n",
        "        ) - 1\n",
        "    )\n",
        "    # transform input prompt into tokens\n",
        "    full_tokens = tokenizer(\n",
        "        prompt + \" \" + data_point[\"output\"] + \"</s>\",\n",
        "        truncation=True,\n",
        "        max_length=CUTOFF_LEN + 1,\n",
        "        padding=\"max_length\",\n",
        "    )[\"input_ids\"][:-1]\n",
        "    return {\n",
        "        \"input_ids\": full_tokens,\n",
        "        \"labels\": [-100] * len_user_prompt_tokens\n",
        "        + full_tokens[len_user_prompt_tokens:],\n",
        "        \"attention_mask\": [1] * (len(full_tokens)),\n",
        "    }\n",
        "\n",
        "# 生成された回答の評価\n",
        "def evaluate(instruction, generation_config, max_len, input=\"\", verbose=True):\n",
        "    \"\"\"\n",
        "    (1) 目標\n",
        "        - この関数は、与えられた入力文字列からモデルの出力を得るために使われる。\n",
        "\n",
        "    (2) 引数：\n",
        "        - instruction: str, モデルに何をさせたいかの説明。\n",
        "        - generation_config: transformers.GenerationConfigオブジェクト、モデルの推論に関連するデコードパラメータを指定する。\n",
        "        - max_len: int, モデルの出力の最大長。\n",
        "        - input: str, モデルが命令を解くために必要な入力文字列、デフォルトは\"\"(入力なし)\n",
        "        - verbose: bool, モードの出力を表示するかどうか、デフォルトはTrue\n",
        "    (3) 戻り値\n",
        "        - output: str, 命令と入力に従ったモードの応答\n",
        "    (4) 例\n",
        "        - 命令が \"ABC\"、入力が \"DEF \"で、128トークン以下の回答をモデルに与えたい場合、この関数を次のように使うことができる：\n",
        "            evaluate(instruction=\"ABC\", generation_config=generation_config, max_len=128, input=\"DEF\")\n",
        "\n",
        "    \"\"\"\n",
        "    # construct full input prompt\n",
        "    prompt = f\"\"\"\\\n",
        "[INST] <<SYS>>\n",
        "あなたは親切なアシスタントだし、唐詩を書くこともうまい。\n",
        "<</SYS>>\n",
        "\n",
        "{instruction}\n",
        "{input}\n",
        "[/INST]\"\"\"\n",
        "    # プロンプトのテキストをモデルが必要とする数値表現に変換する。\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].cuda()\n",
        "    # モデルを使って返信を生成する\n",
        "    generation_output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        max_new_tokens=max_len,\n",
        "    )\n",
        "    # 生成された応答をデコードしてプリントアウトする。\n",
        "    for s in generation_output.sequences:\n",
        "        output = tokenizer.decode(s)\n",
        "        output = output.split(\"[/INST]\")[1].replace(\"</s>\", \"\").replace(\"<s>\", \"\").replace(\"Assistant:\", \"\").replace(\"Assistant\", \"\").strip()\n",
        "        if (verbose):\n",
        "            print(output)\n",
        "\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNxuuclCqFf5"
      },
      "source": [
        "## 微調整前のモデルと推論をダウンロードする"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmfEFM7TNuRC"
      },
      "source": [
        "以下のコードブロックの実行時間は、デフォルトの設定を使用した場合、約 **10**分かかりますが、Colabの状態によって異なる場合があります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFq6WXaBnxYq"
      },
      "source": [
        "## 微調整前の推論\n",
        "まず、ファインチューニングなしのモデルで何ができるかを見てみよう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox5QRpz4NiPg"
      },
      "source": [
        "以下のコードブロックの実行時間は、デフォルトの設定を使用した場合、約2分**かかりますが、Colabの状態によって異なる場合があります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1sFQbHGn3Bw"
      },
      "outputs": [],
      "source": [
        "\"\"\" このセルでコードを変更しないことを推奨します。 \"\"\"\n",
        "\n",
        "model_name = \"Qwen/Qwen3-0.6B\" # 微調整に使用するモデルを設定する。\n",
        "cache_dir = \"./cache\"\n",
        "\n",
        "nf4_config = BitsAndBytesConfig(\n",
        "   load_in_4bit=True,\n",
        "   bnb_4bit_quant_type=\"nf4\",\n",
        "   bnb_4bit_use_double_quant=True,\n",
        "   bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# 指定されたモデル名またはパスから，事前に学習された言語モデルを読み込みます．\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    cache_dir=cache_dir,\n",
        "    quantization_config=nf4_config,\n",
        "    low_cpu_mem_usage = True\n",
        ")\n",
        "\n",
        "# トークナイザーを作成し、終了シンボル(eos_token)を設定します。\n",
        "logging.getLogger('transformers').setLevel(logging.ERROR)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    add_eos_token=True,\n",
        "    cache_dir=cache_dir,\n",
        "    #quantization_config=nf4_config\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# モデル推論のためのデコーディング・パラメータの設定\n",
        "max_len = 128\n",
        "generation_config = GenerationConfig(\n",
        "    do_sample=True,\n",
        "    temperature=0.1,\n",
        "    num_beams=1,\n",
        "    top_p=0.3,\n",
        "    no_repeat_ngram_size=3,\n",
        "    pad_token_id=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LyTYIcDOAkO"
      },
      "source": [
        "以下のコードブロックは、デフォルトの設定を使用した場合、実行に約 **1** 分かかりますが、Colab の状態によって異なる場合があります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJyuoPoO2TCr"
      },
      "outputs": [],
      "source": [
        "\"\"\" このセルでコードを変更しないことを推奨します。 \"\"\"\n",
        "\n",
        "# demo examples\n",
        "test_tang_list = ['会っても別れを告げるのは難しい，東風には力がなく、花はすべて散ってしまった。',\n",
        "                  '重いカーテンの下で、深い喪に服していた，横になってからの夜は長く、澄んでいる。',\n",
        "                  '逃亡の果てに追いかける香りの星，禁断の園は驚きに満ちている。']\n",
        "\n",
        "system_prompt = '以下は唐詩の一行目である。 あなたの知識で判断し、唐詩を簡潔に完成しなさい。'\n",
        "\n",
        "# get the model output for each examples\n",
        "demo_before_finetune = []\n",
        "for tang in test_tang_list:\n",
        "  demo_before_finetune.append(f'モデル入力:\\n{system_prompt}\\n\\n{tang}\\n\\nモデル出力:\\n'+evaluate(system_prompt, generation_config, max_len, tang, verbose = False))\n",
        "\n",
        "# print and store the output to text file\n",
        "for idx in range(len(demo_before_finetune)):\n",
        "  print(f\"Example {idx + 1}:\")\n",
        "  print(demo_before_finetune[idx])\n",
        "  print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Stf_U-9FqPjZ"
      },
      "source": [
        "## 微調整のためのハイパーパラメーターの設定\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2ilhBGhTDtU"
      },
      "outputs": [],
      "source": [
        "\"\"\" このハイパーパラメータで遊んでみることを強くお勧めする。 \"\"\"\n",
        "\n",
        "num_train_data = 1040 # ほとんどの場合, 可能な限り多くのデータを訓練したいでしょう. これにより, モデルがより多様な節を見ることができるようになり, 出力の質が向上しますが, 訓練時間も長くなります.\n",
        "                      # デフォルトのパラメータ(1040)を使用した場合: 微調整に約25分、全セルのフル稼働に約50分かかる。\n",
        "                      # 最大値(5000)を使用した場合: 微調整には約100分かかり, 全セルのフル実行には約120分かかる.\n",
        "\n",
        "\"\"\" これらのハイパーパラメータのいくつかを変更したいかもしれない（必ずしも必要ではない）。 \"\"\"\n",
        "\n",
        "output_dir = \"/content/drive/MyDrive\"  # 結果を出力するディレクトリを設定する（別のディレクトリに結果を保存したい場合は、ここで変更できますが、デフォルトのサブディレクトリ、つまりGoogleドライブに保存することを強くお勧めします）\n",
        "ckpt_dir = \"./exp1\" # モデルのチェックポイントを保存するディレクトリを設定します（モデルのチェックポイントを別のディレクトリに保存したい場合は、ここで変更できます）。\n",
        "num_epoch = 1 # 学習する総エポック数を設定する（数値が大きいほど学習時間が長くなる。colabの無料版を利用する場合、学習時間が長すぎると切断される可能性があるので注意が必要）。\n",
        "LEARNING_RATE = 3e-4 # 学習率を設定する。\n",
        "\n",
        "\n",
        "\"\"\" このパラメータ設定コードを変更しないことを推奨します。 \"\"\"\n",
        "\n",
        "cache_dir = \"./cache\" # キャッシュディレクトリのパスを設定します。\n",
        "from_ckpt = False # チェックポイントからモデルの重みをロードするかどうか, デフォルトはno.\n",
        "ckpt_name = None # 特定のチェックポイントから重みをロードする際に使用するファイル名、デフォルトはなし。\n",
        "dataset_dir = \"/content/drive/MyDrive/Tang_trainingdata_ja.json\" # データセットのディレクトリまたはファイルパスを設定します．\n",
        "logging_steps = 20 # 学習ログを出力するステップ数を定義します。\n",
        "save_steps = 65 # モデルを保存するステップ数を設定します。\n",
        "save_total_limit = 3 # モデルのチェックポイントを最大何回保持するかを制御します。\n",
        "report_to = None # 実験的メトリクスを報告する対象を設定します。\n",
        "MICRO_BATCH_SIZE = 4 # マイクロバッチのサイズを定義する\n",
        "BATCH_SIZE = 16 # バッチのサイズを定義する\n",
        "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE # 各マイクロバッチの累積グラデーションステップ数を計算する\n",
        "CUTOFF_LEN = 256 # テキストカットオフの最大長を設定します.\n",
        "LORA_R = 8 # LORA(Layer-wise Random Attention)のR値を設定します.\n",
        "LORA_ALPHA = 16 # LORAのアルファ値を設定します.\n",
        "LORA_DROPOUT = 0.05 # LORAのドロップアウト率を設定する。\n",
        "VAL_SET_SIZE = 0 # バリデーションセットのサイズを設定します。\n",
        "TARGET_MODULES = [\"q_proj\", \"up_proj\", \"o_proj\", \"k_proj\", \"down_proj\", \"gate_proj\", \"v_proj\"] # ターゲットとなるモジュールを設定する。\n",
        "device_map = \"auto\" # デバイスマップを設定。デフォルトは \"auto\"。\n",
        "world_size = int(os.environ.get(\"WORLD_SIZE\", 1)) # 環境変数 \"WORLD_SIZE \"の値を取得、設定されていない場合はデフォルトで1。\n",
        "ddp = world_size != 1 # world_sizeに基づいて分散データ処理(DDP)を使用するかどうかを判断。world_sizeが1の場合、DDPは使用されない。\n",
        "if ddp:\n",
        "    device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
        "    GRADIENT_ACCUMULATION_STEPS = GRADIENT_ACCUMULATION_STEPS // world_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REMmOD6L4tp9"
      },
      "source": [
        "## 微調整開始\n",
        "以下のコードブロックの実行時間は、デフォルト設定を使用した場合、約**10分**かかりますが、Colabの状態によって異なる場合があります。\n",
        "微調整の方法は、以下のサイトを参考：https://www.datacamp.com/tutorial/fine-tuning-qwen3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkOt2hDhR4lG"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6W-xe7h9ti0x"
      },
      "outputs": [],
      "source": [
        "\"\"\" このセルでコードを変更しないことを推奨します。 \"\"\"\n",
        "os.environ[\"WANDB_MODE\"] = \"offline\" # disables online logging\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\" # disables wandb entirely\n",
        "\n",
        "# create the output directory you specify\n",
        "os.makedirs(output_dir, exist_ok = True)\n",
        "os.makedirs(ckpt_dir, exist_ok = True)\n",
        "\n",
        "# 根據 from_ckpt 標誌，從 checkpoint 載入模型權重\n",
        "if from_ckpt:\n",
        "    model = PeftModel.from_pretrained(model, ckpt_name)\n",
        "\n",
        "# 將模型準備好以使用 INT8 訓練\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# 使用 LoraConfig 配置 LORA 模型\n",
        "config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    target_modules=TARGET_MODULES,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model = get_peft_model(model, config)\n",
        "\n",
        "# トークナイザー 的 パディング トークン を 0に設定する\n",
        "tokenizer.pad_token_id = 0\n",
        "\n",
        "# トレーニングデータのロードと処理\n",
        "with open(dataset_dir, \"r\", encoding = \"utf-8\") as f:\n",
        "    data_json = json.load(f)\n",
        "with open(\"tmp_dataset.json\", \"w\", encoding = \"utf-8\") as f:\n",
        "    json.dump(data_json[:num_train_data], f, indent = 2, ensure_ascii = False)\n",
        "\n",
        "data = load_dataset('json', data_files=\"tmp_dataset.json\", download_mode=\"force_redownload\")\n",
        "\n",
        "# 学習データを学習セットと検証セットに分割する（VAL_SET_SIZEが0より大きい場合）\n",
        "if VAL_SET_SIZE > 0:\n",
        "    train_val = data[\"train\"].train_test_split(\n",
        "        test_size=VAL_SET_SIZE, shuffle=True, seed=42\n",
        "    )\n",
        "    train_data = train_val[\"train\"].shuffle().map(generate_training_data)\n",
        "    val_data = train_val[\"test\"].shuffle().map(generate_training_data)\n",
        "else:\n",
        "    train_data = data['train'].shuffle().map(generate_training_data)\n",
        "    val_data = None\n",
        "\n",
        "# トランスフォーマー・トレーナーによるモデル・トレーニング\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=val_data,\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
        "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "        warmup_steps=50,\n",
        "        num_train_epochs=num_epoch,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        fp16=True,  # 混合精度トレーニングの使用\n",
        "        logging_steps=logging_steps,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=save_steps,\n",
        "        output_dir=ckpt_dir,\n",
        "        save_total_limit=save_total_limit,\n",
        "        ddp_find_unused_parameters=False if ddp else None,  # DDPを使用して勾配更新戦略を制御するかどうか\n",
        "        report_to=report_to,\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "# モデルのキャッシュ機能を無効にする\n",
        "model.config.use_cache = False\n",
        "\n",
        "# PyTorchバージョン2.0以降とWindows以外のシステムを使用している場合のモデルのコンパイル\n",
        "if torch.__version__ >= \"2\" and sys.platform != 'win32':\n",
        "    model = torch.compile(model)\n",
        "\n",
        "# モデルトレーニングの開始\n",
        "trainer.train()\n",
        "\n",
        "# 学習済みモデルを指定したディレクトリに保存する。\n",
        "model.save_pretrained(ckpt_dir)\n",
        "\n",
        "# トレーニング中にウェイトが不足する可能性があるという警告メッセージを表示する。\n",
        "print(\"\\n 上記のキーが見つからないという警告は無視してください :)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKeGb8bRqWux"
      },
      "source": [
        "##  テスト\n",
        "微調整は終わった。調整後のモデルをテストしたい。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8I3stApnTWb"
      },
      "source": [
        "まず、保存した微調整済みモデル（チェックポイント）をロードする必要がある。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ag6GvOCe9Ql"
      },
      "outputs": [],
      "source": [
        "\"\"\" このセルでコードを変更しないことを推奨します。 \"\"\"\n",
        "\n",
        "# find all available checkpoints\n",
        "ckpts = []\n",
        "for ckpt in os.listdir(ckpt_dir):\n",
        "    if (ckpt.startswith(\"checkpoint-\")):\n",
        "        ckpts.append(ckpt)\n",
        "\n",
        "# list all the checkpoints\n",
        "ckpts = sorted(ckpts, key = lambda ckpt: int(ckpt.split(\"-\")[-1]))\n",
        "print(\"all available checkpoints:\")\n",
        "print(\" id: checkpoint name\")\n",
        "for (i, ckpt) in enumerate(ckpts):\n",
        "    print(f\"{i:>3}: {ckpt}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khq-LbNlcdfp"
      },
      "outputs": [],
      "source": [
        "\"\"\" チェックポイントを変更したいと思うかもしれないが、必ずしも必要ではない。\"\"\"\n",
        "\n",
        "id_of_ckpt_to_use = -1 # 推論に使用するチェックポイントのID（前のセルの出力に対応）。\n",
        "                        # デフォルト値の-1は, 上記のチェックポイントのリストの中で \"最後から2番目 \"のチェックポイントを指します.\n",
        "                        # 他のチェックポイントを選択したい場合は, -1をリストにあるチェックポイントIDのどれかに変更します.\n",
        "\n",
        "ckpt_name = os.path.join(ckpt_dir, ckpts[id_of_ckpt_to_use])\n",
        "\n",
        "\"\"\" デコード・パラメータを変更する必要があるかもしれないが、必ずしも必要ではない。 \"\"\"\n",
        "\n",
        "# ここでデコードパラメータを調整することができます。デコードパラメータの詳細な説明については、宿題のスライドを参照してください。\n",
        "max_len = 128 # 生成される返信の最大長。\n",
        "temperature = 0.1 # 生成される返信のランダム性を設定。値が小さいほど返信が安定する。\n",
        "top_p = 0.3 # top-p(核)サンプリングのしきい値.\n",
        "# top_k = 5 # top-kの値を調整することで、生成される返答の多様性を高め、繰り返し単語が 生成されないようにする。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTr0DfVBSekD"
      },
      "source": [
        "以下のコードブロックの実行時間は、デフォルト設定を使用した場合、約2分**かかりますが、Colabの状態によって異なる場合があります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wKVPpMVtkql"
      },
      "outputs": [],
      "source": [
        "\"\"\" このセルでコードを変更しないことを推奨します。 \"\"\"\n",
        "\n",
        "test_data_path = \"/content/drive/MyDrive/Tang_testingdata_ja.json\"\n",
        "output_path = os.path.join(output_dir, \"results.txt\")\n",
        "\n",
        "cache_dir = \"./cache\" # キャッシュディレクトリのパスを設定する.\n",
        "seed = 42 # 結果を再現するためのランダムシードを設定する。\n",
        "no_repeat_ngram_size = 3 # 重複セグメントを生成しないように、no-repeat ngramのサイズを設定する。\n",
        "\n",
        "nf4_config = BitsAndBytesConfig(\n",
        "   load_in_4bit=True,\n",
        "   bnb_4bit_quant_type=\"nf4\",\n",
        "   bnb_4bit_use_double_quant=True,\n",
        "   bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# トークン化器を使用して、モデル名をモデルが読み取り可能な数値表現に変換します。\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    cache_dir=cache_dir,\n",
        "    quantization_config=nf4_config\n",
        ")\n",
        "\n",
        "# 事前学習モデルからモデルをロードし、8ビット整数(INT8)モデルとして設定する。\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=nf4_config,\n",
        "    device_map={'': 0},  # 設定使用的設備，此處指定為 GPU 0\n",
        "    cache_dir=cache_dir\n",
        ")\n",
        "\n",
        "# 指定したチェックポイントからモデルの重みをロードする\n",
        "model = PeftModel.from_pretrained(model, ckpt_name, device_map={'': 0})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vs4_EtnoWA5b"
      },
      "source": [
        "以下のコードブロックの実行時間は、デフォルト設定を使用した場合、約 **4**分かかりますが、Colabの状態によって異なる場合があります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcHoU2X8xRkn"
      },
      "outputs": [],
      "source": [
        "\"\"\" このセルでコードを変更しないことを推奨します。 \"\"\"\n",
        "\n",
        "results = []\n",
        "\n",
        "# ランダム性、ビームサーチ、その他の関連パラメータを含む生成コンフィギュレーションを設定する。\n",
        "generation_config = GenerationConfig(\n",
        "    do_sample=True,\n",
        "    temperature=temperature,\n",
        "    num_beams=1,\n",
        "    top_p=top_p,\n",
        "    # top_k=top_k,\n",
        "    no_repeat_ngram_size=no_repeat_ngram_size,\n",
        "    pad_token_id=2\n",
        ")\n",
        "\n",
        "# テストデータの読み込み\n",
        "with open(test_data_path, \"r\", encoding = \"utf-8\") as f:\n",
        "    test_datas = json.load(f)\n",
        "\n",
        "# 各テストデータに対して予測を行い、結果を保存する。\n",
        "with open(output_path, \"w\", encoding = \"utf-8\") as f:\n",
        "  for (i, test_data) in enumerate(test_datas):\n",
        "      predict = evaluate(test_data[\"instruction\"], generation_config, max_len, test_data[\"input\"], verbose = False)\n",
        "      f.write(f\"{i+1}. {test_data['input']} ★★★ {predict}\\n\")\n",
        "      print(f\"{i+1}. {test_data['input']} ★★★ {predict}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tJT1WINKXV7"
      },
      "source": [
        "## **重要なこと**： 15個の唐詩の結果を提出 .\n",
        "これらの唐詩の結果は \"/drive/content/MyDrive/results.txt \"にあります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sT78LeWaJM4D"
      },
      "source": [
        "## ファインチューニング・モデルとファインチューニングなしのモデルの比較をご覧ください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tx_mZZAZWdiE"
      },
      "source": [
        "ここで、上の「微調整前の推論」で見たのと同じ例で、我々のモデルがどのようなことができるかをチェックする。\n",
        "\n",
        "以下のコードブロックの実行時間は、デフォルトの設定であれば**40**秒程度ですが、Colabの状態によって異なるかもしれません。ここで、上の「微調整前の推論」で見たのと同じ例で、我々のモデルがどのようなことができるかをチェックする。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtDG2WhIWZlS"
      },
      "outputs": [],
      "source": [
        "# test demo examples\n",
        "test_tang_list = ['会っても別れを告げるのは難しい，東風には力がなく、花はすべて散ってしまった。',\n",
        "                  '重いカーテンの下で、深い喪に服していた，横になってからの夜は長く、澄んでいる。',\n",
        "                  '逃亡の果てに追いかける香りの星，禁断の園は驚きに満ちている。']\n",
        "\n",
        "# get the model output for each examples\n",
        "demo_before_finetune = []\n",
        "for tang in test_tang_list:\n",
        "  demo_before_finetune.append(f'モデル入力:\\n{system_prompt}\\n\\n{tang}\\n\\nモデル出力:\\n'+evaluate(system_prompt, generation_config, max_len, tang, verbose = False))\n",
        "\n",
        "# print and store the output to text file\n",
        "for idx in range(len(demo_before_finetune)):\n",
        "  print(f\"Example {idx + 1}:\")\n",
        "  print(demo_before_finetune[idx])\n",
        "  print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wagFeDX4Hpa8"
      },
      "source": [
        "## **重要**： 上記の3つの例は提出しないでください。\n",
        "この3つの例は、微調整前と微調整後のモデルのパフォーマンスを比較するためだけに使用されます。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWgTS13bgnWQ"
      },
      "source": [
        "## 結果のあなたのPCにダウンロードします（ダウンロードフォルダにresult.txtファイル）\n",
        "宿題を終わらせるには、このファイルが必要です。 ブラウザが自動的にダウンロードしない場合は、Google Driveにあります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUDuNQOY4os_"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download(output_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}